# comprehensive_diagnose.py
"""
ç»¼åˆè¯Šæ–­è„šæœ¬ - å®šä½evaluate_table2æ€§èƒ½å·®çš„åŸå› 
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import LAMSegmentationModel
from data.dataset import create_dataloader
from utils.metrics import SegmentationMetrics
from configs.lam_config import config


class PerformanceDiagnoser:
    def __init__(self, checkpoint_path, device='cuda:1'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.checkpoint_path = checkpoint_path

        # åŠ è½½é…ç½®
        config.update_for_dataset('rubber_wood')
        self.config = config

        # åŠ è½½æ¨¡å‹
        self.load_model()

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.val_loader = create_dataloader(
            root_dir=config.rubber_wood_path,
            split='val',
            batch_size=1,
            num_workers=0,
            image_size=config.image_size,
            augmentation=False
        )

        self.metrics = SegmentationMetrics(num_classes=config.num_classes)

    def load_model(self):
        """åŠ è½½æ¨¡å‹å¹¶æ£€æŸ¥å…³é”®ç»„ä»¶"""
        print("ğŸ” åŠ è½½æ¨¡å‹å¹¶æ£€æŸ¥æ¶æ„...")

        try:
            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
            self.model = LAMSegmentationModel(
                backbone_name=config.backbone,
                num_classes=config.num_classes,
                num_tokens=config.num_tokens,
                token_rank=config.token_rank,
                num_groups=config.num_groups,
                use_lsm=True,
                tau=config.tau,
                shared_tokens=True
            )
            self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)
            self.model = self.model.to(self.device)
            self.model.eval()
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

        # æ£€æŸ¥æ¨¡å‹å…³é”®ç»„ä»¶
        return self.check_model_components()

    def check_model_components(self):
        """æ£€æŸ¥æ¨¡å‹å…³é”®ç»„ä»¶æ˜¯å¦æ­£å¸¸"""
        issues = []

        # æ£€æŸ¥LAMæ¨¡å—
        if not hasattr(self.model, 'multi_lam'):
            issues.append("âŒ ç¼ºå°‘multi_lamæ¨¡å—")
        else:
            lam_modules = len(self.model.multi_lam.lams)
            print(f"âœ… LAMæ¨¡å—æ•°é‡: {lam_modules}")

            # æ£€æŸ¥LSMæ˜¯å¦å¯ç”¨
            for i, lam in enumerate(self.model.multi_lam.lams):
                if not lam.use_lsm:
                    issues.append(f"âŒ ç¬¬{i}å±‚LSMæœªå¯ç”¨")

        # æ£€æŸ¥backbone
        if not hasattr(self.model, 'backbone'):
            issues.append("âŒ ç¼ºå°‘backboneæ¨¡å—")
        else:
            print("âœ… Backboneæ¨¡å—æ­£å¸¸")

        if issues:
            print("\n".join(issues))
            return False
        return True

    def diagnose_data_issues(self):
        """è¯Šæ–­æ•°æ®é›†é—®é¢˜"""
        print("\nğŸ” è¯Šæ–­æ•°æ®é›†é—®é¢˜...")

        label_stats = {
            'min': 999, 'max': -1,
            'class_counts': np.zeros(6),
            'problem_files': []
        }

        for batch in tqdm(self.val_loader, desc="æ‰«æéªŒè¯é›†"):
            labels = batch['label'].numpy()[0]
            filename = batch['filename'][0]

            batch_min, batch_max = labels.min(), labels.max()
            label_stats['min'] = min(label_stats['min'], batch_min)
            label_stats['max'] = max(label_stats['max'], batch_max)

            # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
            unique, counts = np.unique(labels, return_counts=True)
            for u, c in zip(unique, counts):
                if 0 <= u < 6:
                    label_stats['class_counts'][u] += c

            # æ£€æŸ¥æ ‡ç­¾èŒƒå›´
            if batch_max >= 6 or batch_min < 0:
                label_stats['problem_files'].append({
                    'file': filename, 'min': batch_min, 'max': batch_max
                })

        # è¾“å‡ºè¯Šæ–­ç»“æœ
        print(f"ğŸ“Š æ ‡ç­¾èŒƒå›´: [{label_stats['min']}, {label_stats['max']}]")
        print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {label_stats['class_counts']}")

        if label_stats['problem_files']:
            print(f"âŒ å‘ç°{len(label_stats['problem_files'])}ä¸ªé—®é¢˜æ–‡ä»¶")
        else:
            print("âœ… æ ‡ç­¾èŒƒå›´æ­£å¸¸")

        return label_stats

    def diagnose_prediction_quality(self, num_samples=10):
        """è¯Šæ–­é¢„æµ‹è´¨é‡"""
        print(f"\nğŸ” è¯Šæ–­é¢„æµ‹è´¨é‡ï¼ˆæŠ½æ ·{num_samples}å¼ ï¼‰...")

        self.metrics.reset()
        sample_results = []

        with torch.no_grad():
            for i, batch in enumerate(tqdm(self.val_loader, desc="è¯„ä¼°é¢„æµ‹")):
                if i >= num_samples:
                    break

                images = batch['image'].to(self.device)
                labels = batch['label'].to(self.device)

                # å‰å‘ä¼ æ’­
                logits = self.model(images, compute_cov_loss=False)
                preds = torch.argmax(logits, dim=1)

                # æ›´æ–°æŒ‡æ ‡
                self.metrics.update(preds.cpu().numpy(), labels.cpu().numpy())

                # åˆ†æå•å¼ å›¾ç‰‡
                pred_np = preds[0].cpu().numpy()
                label_np = labels[0].cpu().numpy()

                # è®¡ç®—å„ç±»åˆ«IoU
                class_ious = []
                for class_id in range(6):
                    intersection = ((pred_np == class_id) & (label_np == class_id)).sum()
                    union = ((pred_np == class_id) | (label_np == class_id)).sum()
                    iou = intersection / (union + 1e-8)
                    class_ious.append(iou)

                sample_results.append({
                    'filename': batch['filename'][0],
                    'class_ious': class_ious,
                    'mean_iou': np.mean(class_ious)
                })

        # åˆ†æç»“æœ
        overall_results = self.metrics.compute()
        print(f"ğŸ“Š æ•´ä½“mIoU: {overall_results['miou']:.4f}")

        # æ‰¾å‡ºæœ€å·®çš„æ ·æœ¬
        worst_samples = sorted(sample_results, key=lambda x: x['mean_iou'])[:3]

        print("\nğŸ”´ æ€§èƒ½æœ€å·®çš„3ä¸ªæ ·æœ¬:")
        for i, sample in enumerate(worst_samples):
            print(f"  {i + 1}. {sample['filename']}: mIoU={sample['mean_iou']:.4f}")
            for class_id, iou in enumerate(sample['class_ious']):
                if iou < 0.3:  # è¯†åˆ«ç‡ä½çš„ç±»åˆ«
                    print(f"    ç±»åˆ«{class_id} IoU: {iou:.4f} âŒ")

        return overall_results, sample_results

    def visualize_failure_cases(self, num_cases=5):
        """å¯è§†åŒ–å¤±è´¥æ¡ˆä¾‹"""
        print(f"\nğŸ” å¯è§†åŒ–{num_cases}ä¸ªå¤±è´¥æ¡ˆä¾‹...")

        os.makedirs('./diagnosis_output', exist_ok=True)
        failure_cases = []

        with torch.no_grad():
            for i, batch in enumerate(tqdm(self.val_loader, desc="å¯»æ‰¾å¤±è´¥æ¡ˆä¾‹")):
                if len(failure_cases) >= num_cases:
                    break

                images = batch['image'].to(self.device)
                labels = batch['label'].to(self.device)

                logits = self.model(images, compute_cov_loss=False)
                preds = torch.argmax(logits, dim=1)

                # è®¡ç®—å•å¼ mIoU
                pred_np = preds[0].cpu().numpy()
                label_np = labels[0].cpu().numpy()

                class_ious = []
                for class_id in range(6):
                    intersection = ((pred_np == class_id) & (label_np == class_id)).sum()
                    union = ((pred_np == class_id) | (label_np == class_id)).sum()
                    iou = intersection / (union + 1e-8)
                    class_ious.append(iou)

                mean_iou = np.mean(class_ious)

                # è®°å½•å¤±è´¥æ¡ˆä¾‹ï¼ˆmIoU < 0.5ï¼‰
                if mean_iou < 0.5:
                    failure_cases.append({
                        'image': images[0].cpu(),
                        'pred': preds[0].cpu(),
                        'label': labels[0].cpu(),
                        'filename': batch['filename'][0],
                        'miou': mean_iou
                    })

        # å¯è§†åŒ–å¤±è´¥æ¡ˆä¾‹
        for i, case in enumerate(failure_cases):
            self.plot_comparison(case, f'./diagnosis_output/failure_case_{i}.png')

        print(f"âœ… ä¿å­˜äº†{len(failure_cases)}ä¸ªå¤±è´¥æ¡ˆä¾‹å¯è§†åŒ–")
        return failure_cases

    def plot_comparison(self, case, save_path):
        """ç»˜åˆ¶é¢„æµ‹å¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # åŸå§‹å›¾åƒ
        img = case['image'].numpy().transpose(1, 2, 0)
        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
        img = np.clip(img, 0, 1)

        axes[0].imshow(img)
        axes[0].set_title(f"Original: {case['filename']}")
        axes[0].axis('off')

        # é¢„æµ‹ç»“æœ
        axes[1].imshow(case['pred'].numpy(), cmap='tab10', vmin=0, vmax=9)
        axes[1].set_title(f"Prediction (mIoU: {case['miou']:.3f})")
        axes[1].axis('off')

        # çœŸå®æ ‡ç­¾
        axes[2].imshow(case['label'].numpy(), cmap='tab10', vmin=0, vmax=9)
        axes[2].set_title("Ground Truth")
        axes[2].axis('off')

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

    def check_training_artifacts(self):
        """æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹äº§ç‰©"""
        print("\nğŸ” æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹...")

        checkpoint_dir = os.path.dirname(self.checkpoint_path)

        # æŸ¥æ‰¾æ‰€æœ‰checkpoint
        checkpoints = []
        for f in os.listdir(checkpoint_dir):
            if f.startswith('checkpoint_epoch_') and f.endswith('.pth'):
                epoch = int(f.split('_')[-1].split('.')[0])
                checkpoints.append((epoch, os.path.join(checkpoint_dir, f)))

        checkpoints.sort()

        if checkpoints:
            print(f"âœ… æ‰¾åˆ°{len(checkpoints)}ä¸ªcheckpoint")
            print(f"   æœ€æ—©: epoch {checkpoints[0][0]}, æœ€æ™š: epoch {checkpoints[-1][0]}")

            # æ£€æŸ¥æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')
            if os.path.exists(best_model_path):
                print("âœ… æ‰¾åˆ°best_model.pth")
                best_checkpoint = torch.load(best_model_path, map_location='cpu')
                if 'best_miou' in best_checkpoint:
                    print(f"âœ… æœ€ä½³mIoU: {best_checkpoint['best_miou']:.4f}")
            else:
                print("âŒ æœªæ‰¾åˆ°best_model.pth")
        else:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒcheckpoint")

    def run_comprehensive_diagnosis(self):
        """è¿è¡Œç»¼åˆè¯Šæ–­"""
        print("=" * 80)
        print("ğŸš€ å¼€å§‹ç»¼åˆæ€§èƒ½è¯Šæ–­")
        print("=" * 80)

        # 1. æ£€æŸ¥æ¨¡å‹æ¶æ„
        if not self.load_model():
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œåœæ­¢è¯Šæ–­")
            return

        # 2. è¯Šæ–­æ•°æ®é›†
        data_stats = self.diagnose_data_issues()

        # 3. è¯Šæ–­é¢„æµ‹è´¨é‡
        overall_results, sample_results = self.diagnose_prediction_quality()

        # 4. å¯è§†åŒ–å¤±è´¥æ¡ˆä¾‹
        failure_cases = self.visualize_failure_cases()

        # 5. æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹
        self.check_training_artifacts()

        # 6. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        self.generate_diagnosis_report(data_stats, overall_results, failure_cases)

        print("=" * 80)
        print("âœ… è¯Šæ–­å®Œæˆï¼æŸ¥çœ‹ diagnosis_report.txt è·å–è¯¦ç»†å»ºè®®")
        print("=" * 80)

    def generate_diagnosis_report(self, data_stats, overall_results, failure_cases):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        report = []

        report.append("=" * 80)
        report.append("ğŸ“‹ æ€§èƒ½è¯Šæ–­æŠ¥å‘Š")
        report.append("=" * 80)

        # æ•°æ®é—®é¢˜
        report.append("\nğŸ“Š æ•°æ®è¯Šæ–­:")
        report.append(f"  æ ‡ç­¾èŒƒå›´: [{data_stats['min']}, {data_stats['max']}]")
        if data_stats['problem_files']:
            report.append(f"  âŒ å‘ç°{len(data_stats['problem_files'])}ä¸ªæ ‡ç­¾é—®é¢˜æ–‡ä»¶")
        else:
            report.append("  âœ… æ ‡ç­¾èŒƒå›´æ­£å¸¸")

        # æ€§èƒ½é—®é¢˜
        report.append(f"\nğŸ“Š æ€§èƒ½è¯Šæ–­:")
        report.append(f"  æ•´ä½“mIoU: {overall_results['miou']:.4f}")
        report.append(f"  æ•´ä½“å‡†ç¡®ç‡: {overall_results['macc']:.4f}")

        if overall_results['miou'] < 0.7:
            report.append("  âŒ æ€§èƒ½ä¸¥é‡ä½äºè®ºæ–‡æ°´å¹³(0.7668)")
        elif overall_results['miou'] < 0.76:
            report.append("  âš ï¸ æ€§èƒ½ç•¥ä½äºè®ºæ–‡æ°´å¹³")
        else:
            report.append("  âœ… æ€§èƒ½è¾¾åˆ°è®ºæ–‡æ°´å¹³")

        # å¤±è´¥æ¡ˆä¾‹åˆ†æ
        report.append(f"\nğŸ”´ å¤±è´¥æ¡ˆä¾‹åˆ†æ:")
        report.append(f"  å‘ç°{len(failure_cases)}ä¸ªä¸¥é‡å¤±è´¥æ¡ˆä¾‹(mIoU < 0.5)")

        if failure_cases:
            for i, case in enumerate(failure_cases[:3]):
                report.append(f"  æ¡ˆä¾‹{i + 1}: {case['filename']} - mIoU: {case['miou']:.3f}")

        # å»ºè®®
        report.append("\nğŸ’¡ ä¿®å¤å»ºè®®:")

        if data_stats['problem_files']:
            report.append("  1. ä¿®å¤æ ‡ç­¾é—®é¢˜æ–‡ä»¶ï¼ˆèŒƒå›´è¶…å‡º[0,5]ï¼‰")

        if overall_results['miou'] < 0.7:
            report.append("  2. é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œç¡®ä¿å®Œæ•´30è½®è®­ç»ƒ")
            report.append("  3. æ£€æŸ¥LSMæ¨¡å—æ˜¯å¦åœ¨ç¬¬äºŒé˜¶æ®µå¯ç”¨")
            report.append("  4. éªŒè¯æ•°æ®é¢„å¤„ç†ä¸è®ºæ–‡ä¸€è‡´")

        if len(failure_cases) > 10:
            report.append("  5. å¤±è´¥æ¡ˆä¾‹è¿‡å¤šï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é›†è´¨é‡")

        report.append("\n" + "=" * 80)

        # ä¿å­˜æŠ¥å‘Š
        with open('./diagnosis_output/diagnosis_report.txt', 'w') as f:
            f.write('\n'.join(report))

        print('\n'.join(report))


def main():
    import argparse

    parser = argparse.ArgumentParser(description='æ€§èƒ½è¯Šæ–­å·¥å…·')
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda:1',
                        help='è¿è¡Œè®¾å¤‡')

    args = parser.parse_args()

    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointä¸å­˜åœ¨: {args.checkpoint}")
        return

    # åˆ›å»ºè¯Šæ–­å™¨
    diagnoser = PerformanceDiagnoser(
        checkpoint_path=args.checkpoint,
        device=args.device
    )

    # è¿è¡Œè¯Šæ–­
    diagnoser.run_comprehensive_diagnosis()


if __name__ == "__main__":
    main()

    """
    
    python evaluate/comprehensive_diagnose.py \
    --checkpoint  /home/user4/æ¡Œé¢/wood-defect/wood-defect-output/checkpoints/best_model.pth \
    --device cuda:1
    """